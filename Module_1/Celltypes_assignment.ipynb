{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OWRsLdrmO0rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment for 2025/11/04\n",
        "You will need to generate a plot and upload it on coospace.\n",
        "\n",
        "Bagi Laura: Humán idegsejtek dendrittípus szerinti ('spiny' 'aspiny' 'sparsely spiny') nyugalmi membránpotenticál (vrest) eloszlásdiagramja.\n",
        "\n",
        "Bélai Emese Krisztina: Humán idegsejtek disease_state szerinti nyugalmi membránpotenticál (vrest) eloszlása (hisztogramja).\n",
        "\n",
        "Gyuricza Gergő Pál: sag érték eloszlása egér jobb és bal agyféltekei neuronokban (structure_hemisphere)\n",
        "\n",
        "Ignácz Rebeka: sag vs. fast_trough_v_short_square pontábrája, denrittipus szerint szinezve (dendrite_type)\n",
        "\n",
        "Illés Patrícia Zsófia: sag vs. vrest denrittipus szerinti pontábrája.\n",
        "\n",
        "Klein Kata:fast_trough_t_short_square eloszlása Vip-IRES-Cre és Pvalb-IRES-Cre transzgenikus egérvonalak szerint (transgenic_line).\n",
        "\n",
        "Környei Viktor: Humán idegsejtek disease_state szerinti fast_trough_v_short_square eloszlása.\n",
        "\n",
        "Kristóffy Zsolt János: bemenő ellenállás (input_resistance_mohm) vs reobázikus áram (rheobase_sweep_number) pontábrája, 3 és 5 rétegi (structure_layer_name) humán sejtekben\n",
        "\n",
        "Lüvi Adél: sag (sag) vs nyugalmi membránpotenciál (vrest) pontábrája, 2/3 és 5 rétegi (structure_layer_name) egér sejtekben\n",
        "\n",
        "Machó Fanni: agykérgi mélység (normalized_depth) vs. bemenő ellenállás (input_resistance_mohm) humán sejtekben\n",
        "\n",
        "Micsinai Erika Manyi: agykérgi mélység (normalized_depth) vs. bemenő ellenállás (input_resistance_mohm) egér sejtekben\n",
        "\n",
        "Nagy Vivien: tüzelési adaptáció (adaptation) eloszlása humán és egér 5. Rétegi (structure_layer_name) sejtekben\n",
        "\n",
        "Pataki Zsombor Botond: agykérgi mélység (normalized_depth) vs. bemenő ellenállás (input_resistance_mohm) human sejtekben\n",
        "\n",
        "Pethő Tibor: Norbert:bemenő ellenállás (input_resistance_mohm) vs fast_trough_v_short_square\n",
        "\n",
        "Sarusi Ákos:nyugalmi membránpotenticál (vrest) eloszlása Vip-IRES-Cre és Pvalb-IRES-Cre transzgenikus egérvonalak szerint (transgenic_line).\n",
        "\n",
        "Sásdi Bálint Bendegúz:sag eloszlása Vip-IRES-Cre és Sst-IRES-Cre transzgenikus egérvonalak szerint (transgenic_line).\n",
        "\n",
        "Szafián Dorottya Anna: sag vs. adaptation denrittipus szerinti pontábrája.\n",
        "\n",
        "Szatmári Rebeka:Humán idegsejtek disease_state szerinti nyugalmi membránpotenticál (vrest) eloszlása.\n",
        "\n",
        "Temple Ramóna: avg_isi érték eloszlása egér jobb és bal agyféltekei neuronokban (structure_hemisphere)\n",
        "\n",
        "Zagyva Renáta: sag vs. adaptation pontábrája human mintakban.\n"
      ],
      "metadata": {
        "id": "ZviO0duhO536"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DZbvYMB3OJ67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa025824-9015-4443-b672-65a869ded17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allensdk imported\n"
          ]
        }
      ],
      "source": [
        "# Step 1: import the Allen SDK\n",
        "try:\n",
        "    import allensdk\n",
        "    print('allensdk imported')\n",
        "except ImportError as e:\n",
        "    !pip install allensdk \"numpy == 1.26.4\" \"pandas == 2.3.0\" \"matplotlib > 3.8.0\" \"statsmodels >= 0.14.4\"\n",
        "\n",
        "# you can check the version of the allensdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Czgy4iuLJdwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f4aab32-2387-4434-d4e4-8160e3c65cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given directory doesn't exist, making it\n",
            "Changed into directory\n",
            "data will be saved to /content/Analyzing-Open-Neuroscience-Data-Course-SZTE/\n"
          ]
        }
      ],
      "source": [
        "# Step 2: set up a directory where you save your data (feel free to use the google drive from the celltypes lesson)\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "save_path = '/content/Analyzing-Open-Neuroscience-Data-Course-SZTE/'   # set this to wherever you want to save your data\n",
        "\n",
        "# check if folder already exists; if not, make it\n",
        "# if it already exists, let us know\n",
        "if os.path.isdir(save_path):\n",
        "   print(\"Given directory exists\")\n",
        "else:\n",
        "   print(\"Given directory doesn't exist, making it\")\n",
        "   Path(save_path).mkdir(parents=True,exist_ok=True)# make the folder so it exists\n",
        "\n",
        "# change into the folder for the session and use it to save our figures\n",
        "os.chdir(save_path)\n",
        "print(\"Changed into directory\")\n",
        "\n",
        "print('data will be saved to {}'.format(save_path))\n",
        "\n",
        "# check out your google drive!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IuK_wUSOJH8T"
      },
      "outputs": [],
      "source": [
        "# Step 3: Import important packages, and redefine the get_sweep function\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py # this is necessary for the get_sweep function below\n",
        "\n",
        "# the get_sweep function reads the .npy files that you download\n",
        "\n",
        "def get_sweep(self, sweep_number):\n",
        "  \"\"\" Retrieve the stimulus, response, index_range, and sampling rate\n",
        "  for a particular sweep.  This method hides the NWB file's distinction\n",
        "  between a \"Sweep\" and an \"Experiment\".  An experiment is a subset of\n",
        "  of a sweep that excludes the initial test pulse.  It also excludes\n",
        "  any erroneous response data at the end of the sweep (usually for\n",
        "  ramp sweeps, where recording was terminated mid-stimulus).\n",
        "\n",
        "  Some sweeps do not have an experiment, so full data arrays are\n",
        "  returned.  Sweeps that have an experiment return full data arrays\n",
        "  (include the test pulse) with any erroneous data trimmed from the\n",
        "  back of the sweep.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  sweep_number: int\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dict\n",
        "      A dictionary with 'stimulus', 'response', 'index_range', and\n",
        "      'sampling_rate' elements.  The index range is a 2-tuple where\n",
        "      the first element indicates the end of the test pulse and the\n",
        "      second index is the end of valid response data.\n",
        "  \"\"\"\n",
        "  with h5py.File(self.file_name, 'r') as f:\n",
        "\n",
        "      swp = f['epochs']['Sweep_%d' % sweep_number]\n",
        "\n",
        "      # fetch data from file and convert to correct SI unit\n",
        "      # this operation depends on file version. early versions of\n",
        "      #   the file have incorrect conversion information embedded\n",
        "      #   in the nwb file and data was stored in the appropriate\n",
        "      #   SI unit. For those files, return uncorrected data.\n",
        "      #   For newer files (1.1 and later), apply conversion value.\n",
        "      major, minor = self.get_pipeline_version()\n",
        "      if (major == 1 and minor > 0) or major > 1:\n",
        "          # stimulus\n",
        "          stimulus_dataset = swp['stimulus']['timeseries']['data']\n",
        "          conversion = float(stimulus_dataset.attrs[\"conversion\"])\n",
        "          stimulus = stimulus_dataset[()] * conversion\n",
        "          # acquisition\n",
        "          response_dataset = swp['response']['timeseries']['data']\n",
        "          conversion = float(response_dataset.attrs[\"conversion\"])\n",
        "          response = response_dataset[()] * conversion\n",
        "      else:   # old file version\n",
        "          stimulus_dataset = swp['stimulus']['timeseries']['data']\n",
        "          stimulus = stimulus_dataset[()]\n",
        "          response = swp['response']['timeseries']['data'][()]\n",
        "\n",
        "      if 'unit' in stimulus_dataset.attrs:\n",
        "          unit = stimulus_dataset.attrs[\"unit\"].decode('UTF-8')\n",
        "\n",
        "          unit_str = None\n",
        "          if unit.startswith('A'):\n",
        "              unit_str = \"Amps\"\n",
        "          elif unit.startswith('V'):\n",
        "              unit_str = \"Volts\"\n",
        "          assert unit_str is not None, Exception(\n",
        "              \"Stimulus time series unit not recognized\")\n",
        "      else:\n",
        "          unit = None\n",
        "          unit_str = 'Unknown'\n",
        "\n",
        "      swp_idx_start = swp['stimulus']['idx_start'][()]\n",
        "      swp_length = swp['stimulus']['count'][()]\n",
        "\n",
        "      swp_idx_stop = swp_idx_start + swp_length - 1\n",
        "      sweep_index_range = (swp_idx_start, swp_idx_stop)\n",
        "\n",
        "      # if the sweep has an experiment, extract the experiment's index\n",
        "      # range\n",
        "      try:\n",
        "          exp = f['epochs']['Experiment_%d' % sweep_number]\n",
        "          exp_idx_start = exp['stimulus']['idx_start'][()]\n",
        "          exp_length = exp['stimulus']['count'][()]\n",
        "          exp_idx_stop = exp_idx_start + exp_length - 1\n",
        "          experiment_index_range = (exp_idx_start, exp_idx_stop)\n",
        "      except KeyError:\n",
        "          # this sweep has no experiment.  return the index range of the\n",
        "          # entire sweep.\n",
        "          experiment_index_range = sweep_index_range\n",
        "\n",
        "      assert sweep_index_range[0] == 0, Exception(\n",
        "          \"index range of the full sweep does not start at 0.\")\n",
        "\n",
        "      return {\n",
        "          'stimulus': stimulus,\n",
        "          'response': response,\n",
        "          'stimulus_unit' : unit_str,\n",
        "          'index_range': experiment_index_range,\n",
        "          'sampling_rate': 1.0 * swp['stimulus']['timeseries']['starting_time'].attrs['rate']\n",
        "      }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "nxODMVohJH8T"
      },
      "outputs": [],
      "source": [
        "#Step 3: download the data\n",
        "from allensdk.core.cell_types_cache import CellTypesCache\n",
        "\n",
        "#Initialize the cache as 'ctc' (cell types cache)\n",
        "ctc = CellTypesCache(manifest_file='cell_types/manifest.json') #\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4:\n",
        "\n",
        "# Download the ephys features, convert to dataframe, get the metadata (cells variable), make a big dataframe (full_dataframe)\n",
        "# hint: you can re-use the code from the class (as always)\n",
        "\n",
        "# ephys_features = ctc.....\n",
        "# ef_df = pd.DataFrame(....\n",
        "# ...\n",
        "# full_dataframe = ...\n",
        "\n"
      ],
      "metadata": {
        "id": "dWOSOoW7TDXL"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "1dlmG_SmJH8V"
      },
      "outputs": [],
      "source": [
        "# Step 5:\n",
        "# make a separate human and mouse dataframe, if it is your assignment (using the species column)\n",
        "\n",
        "#mouse_df = full_dataframe[...\n",
        "#human_df = full_dataframe[...\n",
        "\n",
        "# Step 6:\n",
        "# make further separations if necessary..\n",
        "\n",
        "\n",
        "#Step 7: make a scatter plot, histogram, both, depending on your specific assignment\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}